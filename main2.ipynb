{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:07.849037Z",
     "start_time": "2025-01-09T06:54:01.936139Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import webrtcvad\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vodou\\miniconda3\\envs\\mfcc_test\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:07.864816Z",
     "start_time": "2025-01-09T06:54:07.853006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('C:/Users/vodou/Documents/Python Scripts')\n",
    "import pushover_notifier as pn\n",
    "\n",
    "notifier = pn.PushoverNotifier('Microcontroller_Project')\n",
    "notifier.redirect_print_to_pushover()\n",
    "\"\"\""
   ],
   "id": "be7d06730e3fbe11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport sys\\nsys.path.append('C:/Users/vodou/Documents/Python Scripts')\\nimport pushover_notifier as pn\\n\\nnotifier = pn.PushoverNotifier('Microcontroller_Project')\\nnotifier.redirect_print_to_pushover()\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Import Audio Files\n",
    "\n",
    "### Classes: Real = 0; Fake = 1"
   ],
   "id": "7d52f6ad85708eb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:08.269459Z",
     "start_time": "2025-01-09T06:54:08.256325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_path_fake = \"for-2seconds/testing/fake\"\n",
    "test_path_real = \"for-2seconds/testing/real\"\n",
    "\n",
    "train_path_fake = \"for-2seconds/training/fake\"\n",
    "train_path_real = \"for-2seconds/training/real\"\n",
    "\n",
    "validation_path_fake = \"for-2seconds/validation/fake\"\n",
    "validation_path_real = \"for-2seconds/validation/real\""
   ],
   "id": "3110ada44e466519",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:08.331325Z",
     "start_time": "2025-01-09T06:54:08.316596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []"
   ],
   "id": "e2a4451a0c85756c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:08.624764Z",
     "start_time": "2025-01-09T06:54:08.348554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add fake.wav files to the X_test matrix and generate corresponding y_test vector\n",
    "\n",
    "for audio_file in os.listdir(test_path_fake):\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "\n",
    "        file_path = os.path.join(test_path_fake, audio_file)\n",
    "\n",
    "        sample_rate, audio_data = wavfile.read(file_path)\n",
    "\n",
    "        # Ensure audio is 1D (mono), if stereo, you can take one channel\n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = audio_data[:, 0]  # Take the first channel\n",
    "\n",
    "        X_test.append(audio_data)\n",
    "        y_test.append(1)"
   ],
   "id": "70bf4ec633148d2d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:08.954146Z",
     "start_time": "2025-01-09T06:54:08.642425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add real.wav files to the X_test matrix and generate corresponding y_test vector\n",
    "\n",
    "for audio_file in os.listdir(test_path_real):\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "\n",
    "        file_path = os.path.join(test_path_real, audio_file)\n",
    "\n",
    "        sample_rate, audio_data = wavfile.read(file_path)\n",
    "\n",
    "        # Ensure audio is 1D (mono), if stereo, you can take one channel\n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = audio_data[:, 0]  # Take the first channel\n",
    "\n",
    "        X_test.append(audio_data)\n",
    "        y_test.append(0)"
   ],
   "id": "ef7b2eab5cb45739",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:12.579898Z",
     "start_time": "2025-01-09T06:54:08.972430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add fake.wav files to the X_train matrix and generate corresponding y_train vector\n",
    "\n",
    "for audio_file in os.listdir(train_path_fake):\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "\n",
    "        file_path = os.path.join(train_path_fake, audio_file)\n",
    "\n",
    "        sample_rate, audio_data = wavfile.read(file_path)\n",
    "\n",
    "        # Ensure audio is 1D (mono), if stereo, you can take one channel\n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = audio_data[:, 0]  # Take the first channel\n",
    "\n",
    "        X_train.append(audio_data)\n",
    "        y_train.append(1)"
   ],
   "id": "ddbdd66bf6ba91bb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:14.429164Z",
     "start_time": "2025-01-09T06:54:12.594055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add real.wav files to the X_train matrix and generate corresponding y_train vector\n",
    "\n",
    "for audio_file in os.listdir(train_path_real):\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "\n",
    "        file_path = os.path.join(train_path_real, audio_file)\n",
    "\n",
    "        sample_rate, audio_data = wavfile.read(file_path)\n",
    "\n",
    "        # Ensure audio is 1D (mono), if stereo, you can take one channel\n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = audio_data[:, 0]  # Take the first channel\n",
    "\n",
    "        X_train.append(audio_data)\n",
    "        y_train.append(0)"
   ],
   "id": "7c6fa21a7e581160",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:14.727142Z",
     "start_time": "2025-01-09T06:54:14.446688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add fake.wav files to the X_train matrix and generate corresponding y_train vector\n",
    "\n",
    "for audio_file in os.listdir(validation_path_fake):\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "\n",
    "        file_path = os.path.join(validation_path_fake, audio_file)\n",
    "\n",
    "        sample_rate, audio_data = wavfile.read(file_path)\n",
    "\n",
    "        # Ensure audio is 1D (mono), if stereo, you can take one channel\n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = audio_data[:, 0]  # Take the first channel\n",
    "\n",
    "        X_val.append(audio_data)\n",
    "        y_val.append(1)"
   ],
   "id": "8ed8c84d50de7d3d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:14.982177Z",
     "start_time": "2025-01-09T06:54:14.744245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add real.wav files to the X_val matrix and generate corresponding y_val vector\n",
    "\n",
    "for audio_file in os.listdir(validation_path_real):\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "\n",
    "        file_path = os.path.join(validation_path_real, audio_file)\n",
    "\n",
    "        sample_rate, audio_data = wavfile.read(file_path)\n",
    "\n",
    "        # Ensure audio is 1D (mono), if stereo, you can take one channel\n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = audio_data[:, 0]  # Take the first channel\n",
    "\n",
    "        X_val.append(audio_data)\n",
    "        y_val.append(0)"
   ],
   "id": "ebc74dce9849a41",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert matrices to 2D numpy arrays, and vectors (lists) into 1D numpy arrays",
   "id": "69862f01a2563d6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:15.461288Z",
     "start_time": "2025-01-09T06:54:15.000456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = np.array(X_train)#[:100]\n",
    "X_test = np.array(X_test)#[:100]\n",
    "X_val = np.array(X_val)#[:100]"
   ],
   "id": "132d9bf43e802bcd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:15.492340Z",
     "start_time": "2025-01-09T06:54:15.479074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test = np.array(y_test)#[:100]\n",
    "y_train = np.array(y_train)#[:100]\n",
    "y_val = np.array(y_val)#[:100]"
   ],
   "id": "a9766fe081c75b89",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "6e4a426de7d94e1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:15.552235Z",
     "start_time": "2025-01-09T06:54:15.525978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features(X):\n",
    "    FRAME_LENGTH = 2048\n",
    "    HOP_LENGTH = 512\n",
    "    sample_rate = 44100\n",
    "\n",
    "    zcr_features = []\n",
    "    rms_features = []\n",
    "    mfccs_features = []\n",
    "    chroma_features = []\n",
    "    spectral_centroid_features = []\n",
    "    spectral_bandwidth_features = []\n",
    "    spectral_rolloff_features = []\n",
    "\n",
    "    for audio in X:\n",
    "        # Ensure the audio is in float32 format\n",
    "        audio = audio.astype(np.float32) / 32768.0  # Convert from int16 to float32\n",
    "\n",
    "        # Extract features\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        rms = librosa.feature.rms(y=audio, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sample_rate)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)\n",
    "\n",
    "        # Append features\n",
    "        zcr_features.append(zcr)\n",
    "        rms_features.append(rms)\n",
    "        mfccs_features.append(mfccs)\n",
    "        chroma_features.append(chroma)\n",
    "        spectral_centroid_features.append(spectral_centroid)\n",
    "        spectral_bandwidth_features.append(spectral_bandwidth)\n",
    "        spectral_rolloff_features.append(spectral_rolloff)\n",
    "\n",
    "    return zcr_features, rms_features, mfccs_features,chroma_features, spectral_centroid_features, spectral_bandwidth_features, spectral_rolloff_features\n"
   ],
   "id": "f1f099a0299a7d2d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:54:15.600009Z",
     "start_time": "2025-01-09T06:54:15.587329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combine_features(zcr_list, rms_list, mfccs_list, chroma_list, spectral_centroid_list, spectral_bandwidth_list, spectral_rolloff_list):\n",
    "\n",
    "    zcr_features = np.swapaxes(zcr_list, 1, 2)\n",
    "    rms_features = np.swapaxes(rms_list, 1, 2)\n",
    "    mfccs_features = np.swapaxes(mfccs_list, 1, 2)\n",
    "    chroma_features = np.swapaxes(chroma_list, 1, 2)\n",
    "    spectral_centroid_features = np.swapaxes(spectral_centroid_list, 1, 2)\n",
    "    spectral_bandwidth_features = np.swapaxes(spectral_bandwidth_list, 1, 2)\n",
    "    spectral_rolloff_features = np.swapaxes(spectral_rolloff_list, 1, 2)\n",
    "\n",
    "    X_features = np.concatenate(\n",
    "        (zcr_features, rms_features, mfccs_features, chroma_features, spectral_centroid_features, spectral_bandwidth_features, spectral_rolloff_features),\n",
    "        axis=2)\n",
    "\n",
    "    return X_features"
   ],
   "id": "907894879b2b0dd6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:36.303882Z",
     "start_time": "2025-01-09T06:54:15.618896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "zcr_features_val, rms_features_val, mfccs_features_val, chroma_features_val, spectral_centroid_features_val, spectral_bandwidth_features_val, spectral_rolloff_features_val = extract_features(X_val)\n",
    "zcr_features_test, rms_features_test, mfccs_features_test, chroma_features_test, spectral_centroid_features_test, spectral_bandwidth_features_test, spectral_rolloff_features_test = extract_features(X_test)\n",
    "zcr_features_train, rms_features_train, mfccs_features_train, chroma_features_train, spectral_centroid_features_train, spectral_bandwidth_features_train, spectral_rolloff_features_train = extract_features(X_train)"
   ],
   "id": "ad535cb23ffb9dcf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vodou\\miniconda3\\envs\\mfcc_test\\lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:36.789439Z",
     "start_time": "2025-01-09T07:03:36.479534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_features_val = combine_features(zcr_features_val, rms_features_val, mfccs_features_val, chroma_features_val, spectral_centroid_features_val, spectral_bandwidth_features_val, spectral_rolloff_features_val)\n",
    "X_features_test = combine_features(zcr_features_test, rms_features_test, mfccs_features_test, chroma_features_test, spectral_centroid_features_test, spectral_bandwidth_features_test, spectral_rolloff_features_test)\n",
    "X_features_train = combine_features(zcr_features_train, rms_features_train, mfccs_features_train, chroma_features_train, spectral_centroid_features_train, spectral_bandwidth_features_train, spectral_rolloff_features_train)"
   ],
   "id": "54c92ae091b31b44",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:36.804765Z",
     "start_time": "2025-01-09T07:03:36.795565Z"
    }
   },
   "cell_type": "code",
   "source": "X_features_train.shape",
   "id": "cc6e6a664aa1dce2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13956, 63, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:37.259575Z",
     "start_time": "2025-01-09T07:03:36.870393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = np.array([flattened.flatten() for flattened in X_features_train])\n",
    "x_val = np.array([flattened.flatten() for flattened in X_features_val])\n",
    "x_test = np.array([flattened.flatten() for flattened in X_features_test])"
   ],
   "id": "67228d8865e7cce2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:37.352087Z",
     "start_time": "2025-01-09T07:03:37.341464Z"
    }
   },
   "cell_type": "code",
   "source": "x_train.shape",
   "id": "c3c91dc9793afc71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13956, 1890)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:37.600826Z",
     "start_time": "2025-01-09T07:03:37.468730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "indices = np.random.permutation(len(x_train))\n",
    "\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "indices = np.random.permutation(len(x_val))\n",
    "\n",
    "x_val = x_val[indices]\n",
    "y_val = y_val[indices]\n",
    "\n",
    "indices = np.random.permutation(len(x_test))\n",
    "\n",
    "x_test = x_test[indices]\n",
    "y_test = y_test[indices]"
   ],
   "id": "ea1340d96d41eaa3",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "### Important: cannot prune model because pruning only works on Dense Layers, Conv2D layers, and their 'derivatives', hence we will not be pruning our model."
   ],
   "id": "528ebcd7b4be4b86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:37.725590Z",
     "start_time": "2025-01-09T07:03:37.699421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_model(input_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Reshape input to make it suitable for Conv1D\n",
    "    model.add(Conv1D(filters=12, kernel_size=3, activation='relu', input_shape=(input_shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))  # Max Pooling to reduce dimensionality\n",
    "    model.add(BatchNormalization())  # Batch Normalization\n",
    "\n",
    "    model.add(Conv1D(filters=6, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))  # Max Pooling to reduce dimensionality\n",
    "    model.add(BatchNormalization())  # Batch Normalization\n",
    "\n",
    "    # Flatten the output to feed into Dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(16, activation='relu'))  # Further reduce neurons\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 1st Dense Layer\n",
    "    model.add(Dense(6, activation='relu'))  # Further reduce neurons\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Final Dense Layer for Binary Classification\n",
    "    model.add(Dense(2, activation='softmax'))  # Two output neurons with softmax activation\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "id": "7121324b57b7d521",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:37.820138Z",
     "start_time": "2025-01-09T07:03:37.807481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(X_features_train, X_features_val, y_train, y_val):\n",
    "\n",
    "    model = create_model(X_features_train.shape) # adding one to make input shape even (such that pooling works in UNet)\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes=2)\n",
    "    y_val = to_categorical(y_val, num_classes=2)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint('CustomModel.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_accuracy', factor=0.02, patience=5)\n",
    "\n",
    "    history = model.fit(X_features_train, y_train, epochs=30, batch_size=64, validation_data=(X_features_val, y_val), callbacks=[rlrop, early_stopping, checkpoint])\n",
    "\n",
    "    return model, history"
   ],
   "id": "2e439fb3005d6280",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:03:38.179486Z",
     "start_time": "2025-01-09T07:03:37.900224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = create_model(x_train.shape)\n",
    "model.summary()"
   ],
   "id": "ca131ad049893660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 1888, 12)          48        \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 944, 12)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 944, 12)          48        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 942, 6)            222       \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 471, 6)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 471, 6)           24        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2826)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                45232     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 6)                24        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 14        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,778\n",
      "Trainable params: 45,698\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:19.121530Z",
     "start_time": "2025-01-09T07:03:38.244806Z"
    }
   },
   "cell_type": "code",
   "source": "model, history = train_model(x_train, x_val, y_train, y_val)",
   "id": "cb698ab3b296fdf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.6233 - accuracy: 0.6736\n",
      "Epoch 1: val_loss improved from inf to 0.57830, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 21s 89ms/step - loss: 0.6234 - accuracy: 0.6735 - val_loss: 0.5783 - val_accuracy: 0.7028 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.4619 - accuracy: 0.7985\n",
      "Epoch 2: val_loss improved from 0.57830 to 0.40790, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 18s 83ms/step - loss: 0.4619 - accuracy: 0.7984 - val_loss: 0.4079 - val_accuracy: 0.8025 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.3665 - accuracy: 0.8479\n",
      "Epoch 3: val_loss improved from 0.40790 to 0.35835, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 18s 84ms/step - loss: 0.3665 - accuracy: 0.8479 - val_loss: 0.3584 - val_accuracy: 0.8224 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.8728\n",
      "Epoch 4: val_loss did not improve from 0.35835\n",
      "219/219 [==============================] - 18s 82ms/step - loss: 0.3183 - accuracy: 0.8728 - val_loss: 0.6308 - val_accuracy: 0.7176 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.8871\n",
      "Epoch 5: val_loss improved from 0.35835 to 0.32534, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 18s 82ms/step - loss: 0.2862 - accuracy: 0.8871 - val_loss: 0.3253 - val_accuracy: 0.8645 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.2734 - accuracy: 0.8898\n",
      "Epoch 6: val_loss did not improve from 0.32534\n",
      "219/219 [==============================] - 18s 81ms/step - loss: 0.2738 - accuracy: 0.8896 - val_loss: 0.3678 - val_accuracy: 0.8457 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.2465 - accuracy: 0.9054\n",
      "Epoch 7: val_loss did not improve from 0.32534\n",
      "219/219 [==============================] - 19s 85ms/step - loss: 0.2465 - accuracy: 0.9053 - val_loss: 1.1861 - val_accuracy: 0.6132 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9095\n",
      "Epoch 8: val_loss did not improve from 0.32534\n",
      "219/219 [==============================] - 20s 92ms/step - loss: 0.2379 - accuracy: 0.9096 - val_loss: 1.3511 - val_accuracy: 0.5683 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9172\n",
      "Epoch 9: val_loss did not improve from 0.32534\n",
      "219/219 [==============================] - 19s 88ms/step - loss: 0.2177 - accuracy: 0.9172 - val_loss: 1.2249 - val_accuracy: 0.5803 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9183\n",
      "Epoch 10: val_loss did not improve from 0.32534\n",
      "219/219 [==============================] - 19s 88ms/step - loss: 0.2086 - accuracy: 0.9182 - val_loss: 0.9874 - val_accuracy: 0.6582 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9308\n",
      "Epoch 11: val_loss improved from 0.32534 to 0.19198, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 19s 86ms/step - loss: 0.1847 - accuracy: 0.9308 - val_loss: 0.1920 - val_accuracy: 0.9271 - lr: 2.0000e-05\n",
      "Epoch 12/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9340\n",
      "Epoch 12: val_loss improved from 0.19198 to 0.18700, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 19s 87ms/step - loss: 0.1794 - accuracy: 0.9340 - val_loss: 0.1870 - val_accuracy: 0.9292 - lr: 2.0000e-05\n",
      "Epoch 13/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1853 - accuracy: 0.9320\n",
      "Epoch 13: val_loss improved from 0.18700 to 0.18383, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 19s 85ms/step - loss: 0.1853 - accuracy: 0.9320 - val_loss: 0.1838 - val_accuracy: 0.9331 - lr: 2.0000e-05\n",
      "Epoch 14/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.9371\n",
      "Epoch 14: val_loss did not improve from 0.18383\n",
      "219/219 [==============================] - 19s 87ms/step - loss: 0.1755 - accuracy: 0.9372 - val_loss: 0.1883 - val_accuracy: 0.9321 - lr: 2.0000e-05\n",
      "Epoch 15/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9351\n",
      "Epoch 15: val_loss improved from 0.18383 to 0.18298, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 19s 87ms/step - loss: 0.1765 - accuracy: 0.9351 - val_loss: 0.1830 - val_accuracy: 0.9338 - lr: 2.0000e-05\n",
      "Epoch 16/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9371\n",
      "Epoch 16: val_loss did not improve from 0.18298\n",
      "219/219 [==============================] - 19s 85ms/step - loss: 0.1752 - accuracy: 0.9371 - val_loss: 0.1868 - val_accuracy: 0.9331 - lr: 2.0000e-05\n",
      "Epoch 17/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9376\n",
      "Epoch 17: val_loss did not improve from 0.18298\n",
      "219/219 [==============================] - 19s 86ms/step - loss: 0.1772 - accuracy: 0.9375 - val_loss: 0.1851 - val_accuracy: 0.9342 - lr: 2.0000e-05\n",
      "Epoch 18/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1690 - accuracy: 0.9400\n",
      "Epoch 18: val_loss did not improve from 0.18298\n",
      "219/219 [==============================] - 19s 88ms/step - loss: 0.1693 - accuracy: 0.9400 - val_loss: 0.1919 - val_accuracy: 0.9299 - lr: 2.0000e-05\n",
      "Epoch 19/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1742 - accuracy: 0.9381\n",
      "Epoch 19: val_loss did not improve from 0.18298\n",
      "219/219 [==============================] - 19s 85ms/step - loss: 0.1746 - accuracy: 0.9380 - val_loss: 0.1845 - val_accuracy: 0.9331 - lr: 2.0000e-05\n",
      "Epoch 20/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1686 - accuracy: 0.9383\n",
      "Epoch 20: val_loss did not improve from 0.18298\n",
      "219/219 [==============================] - 20s 91ms/step - loss: 0.1685 - accuracy: 0.9383 - val_loss: 0.2079 - val_accuracy: 0.9225 - lr: 2.0000e-05\n",
      "Epoch 21/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9380\n",
      "Epoch 21: val_loss did not improve from 0.18298\n",
      "219/219 [==============================] - 20s 93ms/step - loss: 0.1728 - accuracy: 0.9379 - val_loss: 0.1849 - val_accuracy: 0.9335 - lr: 2.0000e-05\n",
      "Epoch 22/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.9395\n",
      "Epoch 22: val_loss did not improve from 0.18298\n",
      "219/219 [==============================] - 20s 89ms/step - loss: 0.1693 - accuracy: 0.9395 - val_loss: 0.1895 - val_accuracy: 0.9328 - lr: 2.0000e-05\n",
      "Epoch 23/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9402\n",
      "Epoch 23: val_loss improved from 0.18298 to 0.18260, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 19s 88ms/step - loss: 0.1672 - accuracy: 0.9401 - val_loss: 0.1826 - val_accuracy: 0.9342 - lr: 4.0000e-07\n",
      "Epoch 24/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9393\n",
      "Epoch 24: val_loss improved from 0.18260 to 0.18181, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 20s 91ms/step - loss: 0.1728 - accuracy: 0.9392 - val_loss: 0.1818 - val_accuracy: 0.9342 - lr: 4.0000e-07\n",
      "Epoch 25/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.9382\n",
      "Epoch 25: val_loss did not improve from 0.18181\n",
      "219/219 [==============================] - 23s 104ms/step - loss: 0.1715 - accuracy: 0.9381 - val_loss: 0.1837 - val_accuracy: 0.9349 - lr: 4.0000e-07\n",
      "Epoch 26/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9379\n",
      "Epoch 26: val_loss did not improve from 0.18181\n",
      "219/219 [==============================] - 20s 92ms/step - loss: 0.1716 - accuracy: 0.9379 - val_loss: 0.1845 - val_accuracy: 0.9338 - lr: 4.0000e-07\n",
      "Epoch 27/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1728 - accuracy: 0.9392\n",
      "Epoch 27: val_loss improved from 0.18181 to 0.18070, saving model to CustomModel.keras\n",
      "219/219 [==============================] - 21s 95ms/step - loss: 0.1729 - accuracy: 0.9392 - val_loss: 0.1807 - val_accuracy: 0.9363 - lr: 4.0000e-07\n",
      "Epoch 28/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.9391\n",
      "Epoch 28: val_loss did not improve from 0.18070\n",
      "219/219 [==============================] - 19s 87ms/step - loss: 0.1710 - accuracy: 0.9390 - val_loss: 0.1815 - val_accuracy: 0.9349 - lr: 4.0000e-07\n",
      "Epoch 29/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9366\n",
      "Epoch 29: val_loss did not improve from 0.18070\n",
      "219/219 [==============================] - 19s 86ms/step - loss: 0.1743 - accuracy: 0.9367 - val_loss: 0.1832 - val_accuracy: 0.9338 - lr: 4.0000e-07\n",
      "Epoch 30/30\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9372\n",
      "Epoch 30: val_loss did not improve from 0.18070\n",
      "219/219 [==============================] - 21s 95ms/step - loss: 0.1731 - accuracy: 0.9372 - val_loss: 0.1818 - val_accuracy: 0.9352 - lr: 4.0000e-07\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:19.418781Z",
     "start_time": "2025-01-09T07:13:19.359509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save('trained_model.h5')\n",
    "#model = load_model(\"trained_model.h5\")"
   ],
   "id": "cc826df503a5b0aa",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:20.108083Z",
     "start_time": "2025-01-09T07:13:19.424915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "y_pred = model.predict(x_test)"
   ],
   "id": "e54cf39086aca73a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:20.280652Z",
     "start_time": "2025-01-09T07:13:20.269389Z"
    }
   },
   "cell_type": "code",
   "source": "y_pred",
   "id": "86f49ee5e7a2d993",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3405330e-02, 9.8659462e-01],\n",
       "       [9.9566388e-01, 4.3361960e-03],\n",
       "       [9.2779654e-01, 7.2203338e-02],\n",
       "       ...,\n",
       "       [1.2046626e-03, 9.9879527e-01],\n",
       "       [3.8956394e-04, 9.9961036e-01],\n",
       "       [7.9260415e-01, 2.0739581e-01]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Quantisation",
   "id": "523c27694a5ec54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Convert Keras model to TFLite model",
   "id": "609d01dee7ac6874"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:32.435222Z",
     "start_time": "2025-01-09T07:13:20.453774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS\n",
    "]\n",
    "converter._experimental_lower_tensor_list_ops = False  # Disable lowering tensor list ops\n",
    "\n",
    "tflite_model = converter.convert()"
   ],
   "id": "14e4c761dd95475f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\vodou\\AppData\\Local\\Temp\\tmp6k18vwe2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\vodou\\AppData\\Local\\Temp\\tmp6k18vwe2\\assets\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:36.630278Z",
     "start_time": "2025-01-09T07:13:32.522162Z"
    }
   },
   "cell_type": "code",
   "source": "tf.saved_model.save(model, \"tflite_model\")",
   "id": "d5568e55d224e024",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tflite_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tflite_model\\assets\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:36.646285Z",
     "start_time": "2025-01-09T07:13:36.636276Z"
    }
   },
   "cell_type": "code",
   "source": "open(\"C:/Users/vodou/Documents/Academic/MA1/Machine_Learning_on_Microcontrollers/ML_MCU/tflite_model.tflite\", \"wb\").write(tflite_model)",
   "id": "ade788bea2e52810",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191092"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:36.738498Z",
     "start_time": "2025-01-09T07:13:36.726410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Show the model size for the non-quantized HDF5 model\n",
    "h5_mod = os.path.getsize('trained_model.h5') / 1024\n",
    "print(\"HDF5 Model size without quantization: %d KB\" % h5_mod)\n",
    "\n",
    "# Show the model size for the non-quantized TFLite model\n",
    "tflite_mod = os.path.getsize('tflite_model.tflite') / 1024\n",
    "print(\"TFLite Model size without quantization: %d KB\" % tflite_mod)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in file size by a factor of %f\" % (h5_mod / tflite_mod))"
   ],
   "id": "f510679e32ba3d7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 Model size without quantization: 623 KB\n",
      "TFLite Model size without quantization: 186 KB\n",
      "\n",
      "Reduction in file size by a factor of 3.339334\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:36.848517Z",
     "start_time": "2025-01-09T07:13:36.820254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ],
   "id": "9d5f4b4b19f80f77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.float32'>\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:36.939876Z",
     "start_time": "2025-01-09T07:13:36.927123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        # Reshape to (batch_size, 32000, 1)\n",
    "        reshaped_value = tf.reshape(input_value, (1, x_train.shape[1], 1))\n",
    "        yield [tf.cast(reshaped_value, dtype=tf.float32)]"
   ],
   "id": "876ce087ed7ab457",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:37.034355Z",
     "start_time": "2025-01-09T07:13:37.022555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.INFO)"
   ],
   "id": "dcb4003ac80ee686",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:48.841168Z",
     "start_time": "2025-01-09T07:13:37.115827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "tflite_model_quant_int8 = converter.convert()"
   ],
   "id": "d805acb2ee115352",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\vodou\\AppData\\Local\\Temp\\tmpuvutxn_y\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\vodou\\AppData\\Local\\Temp\\tmpuvutxn_y\\assets\n",
      "C:\\Users\\vodou\\miniconda3\\envs\\mfcc_test\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:48.933613Z",
     "start_time": "2025-01-09T07:13:48.922433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ],
   "id": "8f0fd3b4d6b10fb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:49.027304Z",
     "start_time": "2025-01-09T07:13:49.014540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the quantized model to disk\n",
    "open(\"quantized_model.tflite\", \"wb\").write(tflite_model_quant_int8)\n",
    "\n",
    "print(\"Model was saved at location: %s\" % os.path.abspath('quantized_model.tflite'))"
   ],
   "id": "da3a4d88a1b720eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was saved at location: C:\\Users\\vodou\\Documents\\Academic\\MA1\\Machine_Learning_on_Microcontrollers\\ML_MCU\\quantized_model.tflite\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:49.119438Z",
     "start_time": "2025-01-09T07:13:49.107138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Show the model size for the 8-bit quantized TFLite model\n",
    "tflite_quant_in_kb = os.path.getsize('quantized_model.tflite') / 1024\n",
    "print(\"TFLite Model size with 8-bit quantization: %d KB\" % tflite_quant_in_kb)\n",
    "\n",
    "print(\"TFLite Model size without quantization: %d KB\" % tflite_mod)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in model size by a factor of %f\" % (tflite_mod / tflite_quant_in_kb))"
   ],
   "id": "a43ef47e5447b3d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model size with 8-bit quantization: 54 KB\n",
      "TFLite Model size without quantization: 186 KB\n",
      "\n",
      "Reduction in model size by a factor of 3.397795\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:49.214159Z",
     "start_time": "2025-01-09T07:13:49.201048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global x_test\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = x_test[test_image_index]\n",
    "    test_label = y_test[test_image_index]\n",
    "\n",
    "    if (test_image_index % 1000 == 0):\n",
    "      print(\"Evaluated on %d images.\" % test_image_index)\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions"
   ],
   "id": "20b8ea290bd2eb22",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:49.308676Z",
     "start_time": "2025-01-09T07:13:49.296158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global x_test\n",
    "  global y_test\n",
    "\n",
    "  test_image_indices = range(x_test.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(y_test== predictions) * 100) / len(x_test)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(x_test)))"
   ],
   "id": "1b058c209a5b84a6",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert to C .h file",
   "id": "5e2ba14271dde4dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:49.401934Z",
     "start_time": "2025-01-09T07:13:49.388740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nstatic const unsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'static const unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ],
   "id": "f3d8676cf12dea14",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:13:49.527093Z",
     "start_time": "2025-01-09T07:13:49.468771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path to the TFLite model file\n",
    "tflite_model_quant_int8_file = Path(\"quantized_model.tflite\")\n",
    "\n",
    "# Read the binary content of the TFLite model\n",
    "with open(tflite_model_quant_int8_file, \"rb\") as f:\n",
    "    tflite_model_content = f.read()\n",
    "\n",
    "# Convert the binary data into a C array\n",
    "c_model_name = \"my_tflite_model\"\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_content, c_model_name))"
   ],
   "id": "73206abd4cc2f5bf",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save Test Data for Inference on MCU",
   "id": "2a81b46bf6ee956d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:22:32.616630Z",
     "start_time": "2025-01-09T07:22:32.593043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the test data as numpy arrays\n",
    "np.save('x_test.npy', x_test.astype(np.float32))\n",
    "np.save('y_test.npy', y_test.astype(np.float32))"
   ],
   "id": "76b46e8ccbaa57e0",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:22:33.366708Z",
     "start_time": "2025-01-09T07:22:33.354149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_npy_to_header(file_name, header_name):\n",
    "    # Load the .npy file\n",
    "    data = np.load(file_name)\n",
    "\n",
    "    # Ensure the data is converted to an integer type\n",
    "    data_int = (data * 255).astype(np.uint8)  # Scale and cast to uint8 (if needed)\n",
    "\n",
    "    # Open the header file for writing\n",
    "    with open(header_name, 'w') as f:\n",
    "        f.write('// Generated header file\\n')\n",
    "        f.write(f'#define {header_name.split(\".\")[0]}_LEN {data_int.size}\\n')\n",
    "        f.write('const uint8_t data[] = {\\n')\n",
    "\n",
    "        # Write the data in hexadecimal format\n",
    "        data_flat = data_int.flatten()  # Flatten to 1D\n",
    "        for i, val in enumerate(data_flat):\n",
    "            f.write(f'0x{val:02x}, ')\n",
    "            if (i + 1) % 12 == 0:  # Newline every 12 values for readability\n",
    "                f.write('\\n')\n",
    "        f.write('\\n};\\n')"
   ],
   "id": "42ff4d8ca38b9344",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:22:37.958284Z",
     "start_time": "2025-01-09T07:22:34.227048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage for x_test.npy and y_test.npy\n",
    "convert_npy_to_header('x_test.npy', 'x_test_data.h')\n",
    "convert_npy_to_header('y_test.npy', 'y_test_data.h')"
   ],
   "id": "1dc35c281b44713c",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ad8713e5b96fd2cf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
